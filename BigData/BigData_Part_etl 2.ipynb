{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4caf4db-e16a-4bd1-bbab-74c59fe3f9fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e93b035-0d05-4b0a-9564-55875ba17a6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import  pyspark.sql.functions as F\n",
    "import snowflake.connector\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30ffc502-ed92-4923-9b00-2e0b1f22b1af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c47f551-bcb3-45e8-90df-df4e324c4941",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_procedures = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", infer_schema)\n",
    "    .option(\"header\", first_row_is_header)\n",
    "    .option(\"sep\", delimiter)\n",
    "    .load(\"/FileStore/tables/procedures.csv\")\n",
    ")\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_supplies = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", infer_schema)\n",
    "    .option(\"header\", first_row_is_header)\n",
    "    .option(\"sep\", delimiter)\n",
    "    .load(\"/FileStore/tables/supplies.csv\")\n",
    ")\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_careplans = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", infer_schema)\n",
    "    .option(\"header\", first_row_is_header)\n",
    "    .option(\"sep\", delimiter)\n",
    "    .load(\"/FileStore/tables/careplans.csv\")\n",
    ")\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_devices = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", infer_schema)\n",
    "    .option(\"header\", first_row_is_header)\n",
    "    .option(\"sep\", delimiter)\n",
    "    .load(\"/FileStore/tables/devices.csv\")\n",
    ")\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_payer_transitions = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", infer_schema)\n",
    "    .option(\"header\", first_row_is_header)\n",
    "    .option(\"sep\", delimiter)\n",
    "    .load(\"/FileStore/tables/payer_transitions.csv\")\n",
    ")\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_providers = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", infer_schema)\n",
    "    .option(\"header\", first_row_is_header)\n",
    "    .option(\"sep\", delimiter)\n",
    "    .load(\"/FileStore/tables/providers.csv\")\n",
    ")\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_allergies = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", infer_schema)\n",
    "    .option(\"header\", first_row_is_header)\n",
    "    .option(\"sep\", delimiter)\n",
    "    .load(\"/FileStore/tables/allergies.csv\")\n",
    ")\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_conditions = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", infer_schema)\n",
    "    .option(\"header\", first_row_is_header)\n",
    "    .option(\"sep\", delimiter)\n",
    "    .load(\"/FileStore/tables/conditions.csv\")\n",
    ")\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_encounters = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", infer_schema)\n",
    "    .option(\"header\", first_row_is_header)\n",
    "    .option(\"sep\", delimiter)\n",
    "    .load(\"/FileStore/tables/encounters.csv\")\n",
    ")\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_immunizations = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", infer_schema)\n",
    "    .option(\"header\", first_row_is_header)\n",
    "    .option(\"sep\", delimiter)\n",
    "    .load(\"/FileStore/tables/immunizations.csv\")\n",
    ")\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_medications = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", infer_schema)\n",
    "    .option(\"header\", first_row_is_header)\n",
    "    .option(\"sep\", delimiter)\n",
    "    .load(\"/FileStore/tables/medications.csv\")\n",
    ")\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_organizations = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", infer_schema)\n",
    "    .option(\"header\", first_row_is_header)\n",
    "    .option(\"sep\", delimiter)\n",
    "    .load(\"/FileStore/tables/organizations.csv\")\n",
    ")\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_patients = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", infer_schema)\n",
    "    .option(\"header\", first_row_is_header)\n",
    "    .option(\"sep\", delimiter)\n",
    "    .load(\"/FileStore/tables/patients.csv\")\n",
    ")\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_payers = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", infer_schema)\n",
    "    .option(\"header\", first_row_is_header)\n",
    "    .option(\"sep\", delimiter)\n",
    "    .load(\"/FileStore/tables/payers.csv\")\n",
    ")\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_imaging_studies = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", infer_schema)\n",
    "    .option(\"header\", first_row_is_header)\n",
    "    .option(\"sep\", delimiter)\n",
    "    .load(\"/FileStore/tables/imaging_studies.csv\")\n",
    ")\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_observations= (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", infer_schema)\n",
    "    .option(\"header\", first_row_is_header)\n",
    "    .option(\"sep\", delimiter)\n",
    "    .load(\"/FileStore/tables/observations.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a3e5f3-5bcd-4690-aa30-cb3cae3e3fc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SnowflakePipeline:\n",
    "    def __init__(self, account, user, password, warehouse, database, schema):\n",
    "        self.account = account\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.warehouse = warehouse\n",
    "        self.database = database\n",
    "        self.schema = schema\n",
    "        self.connection = None\n",
    "        try:\n",
    "            self.connection = snowflake.connector.connect(\n",
    "                account=self.account,\n",
    "                user=self.user,\n",
    "                password=self.password,\n",
    "                warehouse=self.warehouse,\n",
    "                database=self.database,\n",
    "                schema=self.schema,\n",
    "            )\n",
    "            print(\"Conecção feita com sucesso!\")\n",
    "        except Exception as i:\n",
    "            print(f\"Erro de conecção: {str(i)}\")\n",
    "            self.connection = None\n",
    "    def read_table(self, table_name, query=None, Full=False):\n",
    "            if not self.connection:\n",
    "                self.connect_to_snowflake()  # You need to define this method\n",
    "            try:\n",
    "                read_options = {\n",
    "                    \"sfURL\": self.account + \".snowflakecomputing.com\",\n",
    "                    \"user\": self.user,\n",
    "                    \"password\": self.password,\n",
    "                    \"warehouse\": self.warehouse,\n",
    "                    \"database\": self.database,\n",
    "                    \"schema\": self.schema,\n",
    "                }\n",
    "                if not Full:\n",
    "                    read_options[\"dbtable\"] = table_name  # Corrected variable name\n",
    "                else:\n",
    "                    read_options[\"query\"] = query\n",
    "                df = spark.read.format(\"snowflake\").options(**read_options).load()\n",
    "                return df\n",
    "            except Exception as i:\n",
    "                print(\"Erro ao ler tabela:\", i)\n",
    "                return None\n",
    "    def write_table(self, df, table_name):\n",
    "        if self.connection:\n",
    "            try:\n",
    "                start_time = datetime.now()\n",
    "                df.write.format(\"snowflake\").option(\n",
    "                    \"sfURL\", self.account + \".snowflakecomputing.com\"\n",
    "                ).option(\"sfUser\", self.user).option(\n",
    "                    \"sfPassword\", self.password\n",
    "                ).option(\n",
    "                    \"sfDatabase\", self.database\n",
    "                ).option(\n",
    "                    \"sfSchema\", self.schema\n",
    "                ).option(\n",
    "                    \"sfWarehouse\", self.warehouse\n",
    "                ).option(\n",
    "                    \"dbtable\", table_name\n",
    "                ).mode(\n",
    "                    \"overwrite\"\n",
    "                ).save()\n",
    "\n",
    "\n",
    "                end_time = datetime.now()\n",
    "                elapsed_time = end_time - start_time\n",
    "                \n",
    "\n",
    "                cursor = self.connection.cursor()\n",
    "                cursor.execute(f\"DESCRIBE TABLE {table_name}\")\n",
    "                table_columns = cursor.fetchall()\n",
    "                num_columns = len(table_columns)\n",
    "                cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "                num_rows = cursor.fetchone()[0]\n",
    "                cursor.close()\n",
    "                return {\n",
    "                    \"time_to_create_table\": str(elapsed_time),\n",
    "                    \"schema\": self.schema,\n",
    "                    \"table_name\": table_name,\n",
    "                    \"num_columns\": num_columns,\n",
    "                    \"num_rows\": num_rows,\n",
    "                }\n",
    "            except Exception as i:\n",
    "                return f\"Erro a criar tabela: {str(i)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4573c7d9-9a73-44c0-a227-aea4f15a0b77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conecção feita com sucesso!\n"
     ]
    }
   ],
   "source": [
    "snowflake_handler = SnowflakePipeline(\n",
    "    account='eirljxx-tm85236',\n",
    "    user='JOAOBRASIL',\n",
    "    password='Brasil5+',\n",
    "    warehouse='COMPUTE_WH',\n",
    "    database='TRABALHO_FINAL',\n",
    "    schema='TF_PART_1',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11116b9b-b306-44eb-bb2f-1f1c22fc02ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[39]: {'time_to_create_table': '0:00:48.956178',\n 'schema': 'TF_PART_1',\n 'table_name': 'df_observations',\n 'num_columns': 8,\n 'num_rows': 1659750}"
     ]
    }
   ],
   "source": [
    "snowflake_handler.write_table(df_observations, 'df_observations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c6cb19-0909-4fe3-b782-d99e28d5aafc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[40]: {'time_to_create_table': '0:00:09.357874',\n 'schema': 'TF_PART_1',\n 'table_name': 'df_patients',\n 'num_columns': 25,\n 'num_rows': 12352}"
     ]
    }
   ],
   "source": [
    "snowflake_handler.write_table(df_patients, 'df_patients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac0d5da5-2154-4ae3-a5ae-f18c3d07a56d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[41]: {'time_to_create_table': '0:00:10.158682',\n 'schema': 'TF_PART_1',\n 'table_name': 'df_conditions',\n 'num_columns': 6,\n 'num_rows': 114544}"
     ]
    }
   ],
   "source": [
    "snowflake_handler.write_table(df_conditions, 'df_conditions')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "JoaoBrasil_Part_1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
